\documentclass{rapport}
\usepackage{lipsum}
\usepackage{tikz}
\usepackage{wrapfig}
\usepackage{caption}
\title{Rapport UCL - Template} %Titre du fichier

\begin{document}

%----------- Informations du rapport ---------

\unif{EURECOM}
\titre{S5 PROJECT REPORT :\\ 
\begin{center}
\includegraphics[width=0.3\linewidth]{logo_visuwalk.png} 
\end{center}VISUWALK} %Titre du fichier .pdf
% \cours{Nom du cours} %Nom du cours
% \sujet{Sujet du rapport} %Nom du sujet


%----------- Initialisation -------------------
        
\fairemarges %Afficher les marges
\fairepagedegarde %Créer la page de garde
\tableofcontents%Créer la table de matières
\newpage

%------------ Corps du rapport ----------------
\section{Abstract}

For this semester project, to aid the 2.2 billion visually impaired persons in the world, we were asked to develop a system capable of guiding the latter following a line.\\
The tools we were given are a Raspberry Pi and its camera. The main goals of this project are the following :\\
detect the line from the Raspberry Pi’s camera\\
process the information given by the line\\
generate sound cues to guide the user\\

\section {User manual}

\section{Technical manual}

\subsection{Introduction to the Raspberry Pi and Matlab/Simulink
}
    
The first steps towards this project were to connect the Raspberry Pi to our computers and to discover Matlab, which were not easy tasks.
The whole installation was quite lengthy, and did not work for most of us. And if it did work, we then had to connect the camera to the Raspberry Pi, which was also quite hard. So hard that we even thought our camera was broken, until we tested it on other computers, which took a lot of time. In the end, we were left with only 2 Raspberry Pi connected and 2 cameras (Alexandre bought one himself).
Concerning Matlab and Simulink, some of us had already used these softwares, but only on surface-level, so we pretty much had to learn from scratch how to use them and more specifically, how to use them with the Raspberry Pi. And this last part was complicated, leading us to this next paragraph.

\subsection{Choosing between Python and Matlab/Simulink
}

The option of using Python showed up because other groups had issues just like us and switched to Python. Thus, we talked about the switch, and decided not to switch completely as we had already advanced on the Matlab code. Instead, we let Guillaume,in parallel, try to do the video processing part (that was done on Matlab) on Python, to see how easier it would be.
It was much easier and produced better results.And so we were left with one last question : should we also do the sound generation part on Python ?
We were quite reluctant at first, given that almost the whole code was already done in Simulink and that switching and translating it all would take an important amount of time. Besides, we searched and found multiple ways to make a bridge between Python and Simulink. But time was ticking and we still had not found a way to make any of these bridges work correctly, so we made the switch. And it did not take as long as we expected, despite the issues we encountered. 
Still, we found some interesting features like the C code generation, which would be used to embed the program on a portable system. Knowing that shared memory in C is possible and probably easier than in Matlab, we searched for a way to use shared memory in Python, and to manually modify the C generated code to implement the share of memory between the sound generation part and the Python video processing, so that the second could indicate the direction to the first.

\subsection{Image processing part}

The first step toward computing the image and giving an angle is the image acquisition. We use the python library \verb|picamera| in order to get the camera's images, at a quite low quality, because it has proven to be more time-efficient.

% Gui tu peux changer le wrapfigure par des colonnes ou tout ce que tu veux
\begin{center}
\begin{tikzpicture}[scale=2]
\draw[-latex] (0,0) -- (1,0);
\draw[-latex] (0,0) -- (0,-1);
\draw (0.1,-0.1) rectangle (0.9, -0.9);
\draw (0.5,-0.5) node {image};
\draw (1,0) node[right] {\(x\)};
\draw (0,-1) node[below] {\(y\)};
\end{tikzpicture}
\captionof{figure}{OpenCV x and y axis representation}
\end{center}


The frames are then read by openCV and converted into grey. OpenCV represents images with matrices whose indexes may also be seen as coordinate of the following base. An image pixel is then characterized by its coordinate in the following order: \verb|image[y,x]| (matrix line, matrix column).

We then apply a bilateral filter to the image, as it is a filter that preserve the sharp edges and smooth the rest of the image: this is exactly what we need, especially as there is a pattern with lines on the carpet of the Marconi amphiteater that needs to be removed.

% inserer capture de l'image filtree ?

After having a clear image of the line, we apply the Hough transform which will give us a set of straight lines detected on the image. But before calculating any angle, we need to reorient the lines accordingly to our purpose. Indeed, openCV will return us vectors that are oriented towards increasing \(x\), but we need to have them all in the same directon as \(y\), because the user moves on the y axis.

% place le ou tu veux comme tu veux Gui
\begin{center}
\begin{tikzpicture}[scale=2]
  \draw[-latex] (0,-0.6) -- (0,-1.6);
  \draw (0,-1.6) node[below] {\(y\)};

  \draw (0.7,-0.3) -- (1.2,-1.1);
  \draw (1.2,-1.5) -- (0.7,-2.3);

  \draw[->] (1.4, -0.6) -- (1.6, -0.6);
  \draw[->] (1.4, -1.9) -- (1.6, -1.9);

  \draw[->] (2,-0.3) -- (2.5,-1.1);
  \draw[<-] (2.5,-1.5) -- (2,-2.3);

  \draw[->] (2.7, -1.9) -- (2.9, -1.9);

  \draw[->] (3.7,-1.5) -- (3.2,-2.3);
\end{tikzpicture}
\captionof{figure}{popo}
\end{center}

\documentclass{rapport}
\usepackage{lipsum}
\usepackage{tikz}
\usepackage{wrapfig}
\usepackage{caption}
\title{Rapport UCL - Template} %Titre du fichier

\begin{document}

%----------- Informations du rapport ---------

\unif{EURECOM}
\titre{S5 PROJECT REPORT :\\ 
\begin{center}
\includegraphics[width=0.3\linewidth]{logo_visuwalk.png} 
\end{center}VISUWALK} %Titre du fichier .pdf
% \cours{Nom du cours} %Nom du cours
% \sujet{Sujet du rapport} %Nom du sujet


%----------- Initialisation -------------------
        
\fairemarges %Afficher les marges
\fairepagedegarde %Créer la page de garde
\tableofcontents%Créer la table de matières
\newpage

%------------ Corps du rapport ----------------
\section{Abstract}

For this semester project, to aid the 2.2 billion visually impaired persons in the world, we were asked to develop a system capable of guiding the latter following a line.\\
The tools we were given are a Raspberry Pi and its camera. The main goals of this project are the following :\\
detect the line from the Raspberry Pi’s camera\\
process the information given by the line\\
generate sound cues to guide the user

\section {User manual}


\section{Technical manual}

\subsection{Introduction to the Raspberry Pi and Matlab/Simulink
}
    
The first steps towards this project were to connect the Raspberry Pi to our computers and to discover Matlab, which were not easy tasks.
The whole installation was quite lengthy, and did not work for most of us. And if it did work, we then had to connect the camera to the Raspberry Pi, which was also quite hard. So hard that we even thought our camera was broken, until we tested it on other computers, which took a lot of time. In the end, we were left with only 2 Raspberry Pi connected and 2 cameras (Alexandre bought one himself).\\
Concerning Matlab and Simulink, some of us had already used these softwares, but only on surface-level, so we pretty much had to learn from scratch how to use them and more specifically, how to use them with the Raspberry Pi. And this last part was complicated, leading us to this next paragraph.

\subsection{Choosing between Python and Matlab/Simulink
}

The option of using Python showed up because other groups had issues just like us and switched to Python. Thus, we talked about the switch, and decided not to switch completely as we had already advanced on the Matlab code. Instead, we let Guillaume,in parallel, try to do the video processing part (that was done on Matlab) on Python, to see how easier (or harder) it would be.\\
It was much easier and produced better results.And so we were left with one last question : should we also do the sound generation part on Python ?
We were quite reluctant at first, given that almost the whole code was already done in Simulink and that switching and translating it all would take an important amount of time. Besides, we searched and found multiple ways to make a bridge between Python and Simulink. But time was ticking and we still had not found a way to make any of these bridges work correctly, so we made the switch. And it did not take as long as we expected, despite the issues we encountered. \\
Still, we found some interesting features like the C code generation, which would be used to embed the program on a portable system. Knowing that shared memory in C is possible and probably easier than in Matlab, we searched for a way to use shared memory in Python, and to manually modify the C generated code to implement the share of memory between the sound generation part and the Python video processing, so that the second could indicate the direction to the first.

\subsection{Image processing part}

The first step toward computing the image and giving an angle is the image acquisition. We use the python library \verb|picamera| in order to get the camera's images, at a quite low quality, because it has proven to be more time-efficient.\\
\begin{center}
\begin{tikzpicture}[scale=3]
\draw[-latex] (0,0) -- (1,0);
\draw[-latex] (0,0) -- (0,-1);
\draw (0.1,-0.1) rectangle (0.9, -0.9);
\draw (0.5,-0.5) node {image};
\draw (1,0) node[right] {\(x\)};
\draw (0,-1) node[below] {\(y\)};
\end{tikzpicture}
\captionof{figure}{OpenCV x and y axis representation}
\end{center}
The frames are then read by openCV and converted into grey. OpenCV represents images with matrices whose indexes may also be seen as coordinate of the following base. An image pixel is then characterized by its coordinate in the following order: \verb|image[y,x]| (matrix line, matrix column).\\We then apply a bilateral filter to the image, as it is a filter that preserve the sharp edges and smooth the rest of the image: this is exactly what we need, especially as there is a pattern with lines on the carpet of the Marconi amphiteater that needs to be removed and finally, we apply a canny edge detector, giving us the following image:\\
\begin{center}
\includegraphics[width=0.35\linewidth]{canny_edge.jpg}
\captionof{figure}{Fully filtered image of the line}
\end{center}
After having a clear image of the line, we apply the Hough transform which will give us a set of straight lines detected on the image. But before calculating any angle, we need to reorient the lines accordingly to our purpose. Indeed, openCV will return us vectors that are oriented towards increasing \(x\), but we need to have them all in the same directon as \(y\), because the user moves on the y axis.

% place le ou tu veux comme tu veux Gui
\begin{center}
    \begin{tikzpicture}[scale=2]
  \draw[-latex] (0,0) -- (0,-1);
  \draw (0,-1) node[below] {\(y\)};

  \draw (0.7,-0.3) -- (1.2,-1.1);
  \draw (1.2,-1.5) -- (0.7,-2.3);

  \draw[->] (1.4, -0.6) -- (1.6, -0.6);
  \draw[->] (1.4, -1.9) -- (1.6, -1.9);

  \draw[->] (2,-0.3) -- (2.5,-1.1);
  \draw[<-] (2.5,-1.5) -- (2,-2.3);

  \draw[->] (2.7, -1.9) -- (2.9, -1.9);

  \draw[->] (3.7,-1.5) -- (3.2,-2.3);
\end{tikzpicture}
\captionof{figure}{popo}
\end{center}
Afterwards, we consider only the bottom of the frame and we compute the \textit{oriented} angle of each vector \(v_i\) with the \(y\) axis (defined as the \(\alpha\) angle), using:
\[ {\alpha}_i = \text{sgn}(x) \text{arcos}\left(\frac{y_i}{\|v_i\|}\right) \]
This angle is effectively the one that has our interest, since it is the same if we flip the vectors in the other direction along the \(y\) axis.
Then we calculate the average of the \(\alpha_i\) angles, which gives us our final \(\alpha\).\\
In the case the user is not on the line, we had the following strategy to get him back to it: we calculate the angle between the user and a target zone of the line where we want the person to go. As the evaluation is a race, and as a failure could not worsen anything by itself (we count the number of failures and not their durations), we put the target zone a little bit ahead of the line, so that the user gets back to it quickly. This angle has been called the \(\beta\) angle.\\
\begin{center}
    \includegraphics[width=0.3\linewidth]{angle_beta_barycentre_targetpoint.png}
    \captionof{figure}{Representation of the \(\beta\) angle}
\end{center}
In order to compute it, we take a portion of the image vertically, and look at the barycentre of the pixels that were detected as an edge. Then we have a vector between the barycentre and the user (bottom middle horizontally), which angle with the \(y\) axis is computed the same way as for the \(\alpha_i\).\\We have to know when to take the \(\beta\) angle into account. For this, we compute the distance between the user and the bottom of the line, parameter we call \(d\). The method is the same as for the target zone: we compute the barycentre of a portion of the pixels that were detected as edges, and then we just count the distance in pixels between the middle of the image and this barycentre (only along the \(x\) axis).\\
\begin{center}
    \includegraphics[width=0.3\linewidth]{distance_bas_barycentre.png}
    \captionof{figure}{Representation of the distance \(d\)}
\end{center}
If there is no line in the portion where we look for it, we then only take the \(beta\) angle into account, because that means that the user is very far from the line and should first get back to it. Otherwise, we mix the \(\alpha\) and the \(\beta\) angle relatively to the \(d\) parameter, so that the further the user is from the line, the more we take the second angle into account.\\

We use a \(d\)-dependant weight function, which is

\[ f(d) = (1 - e^{\frac{-d}{\tau}}) \]

We choose \(\tau\) so that when we are at the image extremity, the value of \(f\) is \(0.95\). Thus we have
\[ 3\tau = \frac{\text{width}}{2}, \quad \tau = \frac{\text{width}}{6} \]
It follows,
\[f(d) = 1 - e^{\frac{-6d}{w}}\]

Then, for the angle communicated to the user, called \(\gamma\), we choose this expression to have the behaviour we explained sooner.

\[ \gamma = (1 - f(d))\alpha + f(d)\beta \]

Hence we have,

\[ \gamma = \alpha + (1 - e^{\frac{-6d}{w}})(\beta - \alpha) \]\\
Before sending \(\gamma\) to the sound generation module, we bound the value of the angle in \([-70°, 70°]\). We also reserve \(\gamma = 100\) when no line is detected on the image. Then we send the angle.\\
\textit{Note:} would we have a cover to fix the raspberry on the body, could we better tune the position of the zones we look at, because we know the Raspberry's camera aperture, and giving the average height of a body, we know which distance we cover with camera and what pixel corresponds to what position in real life.

\subsection{Description of source files directory}

In the folder of the source code archive, you will find the \verb|rasp_prod| folder, containing the production source code for the raspberry device, the \verb|improc| and \verb|soundproc| folder containing our test codes for development, and the case folder containing some aborted concept protection and fixation covers for the raspberry. (...)

\section{Conclusion}

We achieved the main goals of this project. Our system is simple to use and is able to detect the guideline, to calculate the position of the customer relatively to the guideline. and to guide efficiently the person equipped with it through the course. \\
We still can make some improvements to our system. Firstly we thought it was important for the customer to be able to have means to maintain the system stable . It will make the experience of the customer more comfortable and will optimize the performance of our system. We thought about modeling a 3D printed PLA case that the customer could put easily on his belt so he can run without thinking about his device; but more specifically, that would have fixed the camera orientation, which is good as it is quite an important parameter in the software. We can also produce a nicer sound. Indeed the customer will have our sound continuously in his ears during his running session so it needs to be as pleasant as possible to hear. \\
To reduce the ecological impact of our systems and to make it more accessible to everyone we thought about putting our source code in open source so that everyone can reproduce it. Doing this we will also reduce the carbon footprint of our product since we will less use transport means to sell Visuwalk all around the world. As a company we will still make benefits by providing technical support to people who want to implement the device by themselves. Moreover we will also provide a repair service to lengthen the lifetime of the devices using Visuwalk. Thus Visuwalk will be a project which will fit in a circular economy dynamic.


\end{document}


\subsection{Description of source files directory}

In the folder of the source code archive, you will find the \verb|rasp_prod| folder, containing the production source code for the raspberry device, the \verb|improc| and \verb|soundproc| folder containing our test codes for development, and the case folder containing some aborted concept protection and fixation covers for the raspberry. (...)



\end{document}
